{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><br>One Hot Encoding</h1></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\ghost\\anaconda3\\envs\\daks\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\ghost\\anaconda3\\envs\\daks\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\ghost\\anaconda3\\envs\\daks\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ghost\\anaconda3\\envs\\daks\\lib\\site-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ghost\\anaconda3\\envs\\daks\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\ghost\\anaconda3\\envs\\daks\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: gensim in c:\\users\\ghost\\anaconda3\\envs\\daks\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\ghost\\anaconda3\\envs\\daks\\lib\\site-packages (from gensim) (1.24.3)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\ghost\\anaconda3\\envs\\daks\\lib\\site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\ghost\\anaconda3\\envs\\daks\\lib\\site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: wrapt in c:\\users\\ghost\\anaconda3\\envs\\daks\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.12.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = ['road','sweet','computer','AI']\n",
    "numbers = {'road':1, 'sweet':2, 'computer':3, 'AI':4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'road': 1, 'sweet': 2, 'computer': 3, 'AI': 4}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  team  points\n",
      "0    A      25\n",
      "1    A      12\n",
      "2    B      15\n",
      "3    B      14\n",
      "4    B      19\n",
      "5    B      23\n",
      "6    C      25\n",
      "7    C      29\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#create DataFrame\n",
    "df = pd.DataFrame({'team': ['A', 'A', 'B', 'B', 'B', 'B', 'C', 'C'],\n",
    "                   'points': [25, 12, 15, 14, 19, 23, 25, 29]})\n",
    "\n",
    "#view DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  team  points    0    1    2\n",
      "0    A      25  1.0  0.0  0.0\n",
      "1    A      12  1.0  0.0  0.0\n",
      "2    B      15  0.0  1.0  0.0\n",
      "3    B      14  0.0  1.0  0.0\n",
      "4    B      19  0.0  1.0  0.0\n",
      "5    B      23  0.0  1.0  0.0\n",
      "6    C      25  0.0  0.0  1.0\n",
      "7    C      29  0.0  0.0  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#creating instance of one-hot-encoder\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "#perform one-hot encoding on 'team' column \n",
    "encoder_df = pd.DataFrame(encoder.fit_transform(df[['team']]).toarray())\n",
    "\n",
    "#merge one-hot encoded columns back with original DataFrame\n",
    "final_df = df.join(encoder_df)\n",
    "\n",
    "#view final df\n",
    "print(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><br>Bag of WOrds (BoW)</h1></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating frequency distribution of words using nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"Achievers achievers achievers are not afraid of Challenges, rather they relish them, thrive in them, use them.\n",
    "        Challenges makes is stronger.\n",
    "        Challenges makes us uncomfortable. \n",
    "        If you get comfortable with uncomfort then you will grow. \n",
    "        Challenge the challenge \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the sentences from the text corpus\n",
    "tokenized_text=sent_tokenize(text)#using CountVectorizer and removing stopwords in english language\n",
    "cv1= CountVectorizer(lowercase=True,stop_words='english')#fitting the tonized senetnecs to the countvectorizer\n",
    "text_counts=cv1.fit_transform(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'achievers': 0, 'afraid': 1, 'challenges': 3, 'relish': 7, 'thrive': 9, 'use': 12, 'makes': 6, 'stronger': 8, 'uncomfortable': 11, 'comfortable': 4, 'uncomfort': 10, 'grow': 5, 'challenge': 2}\n",
      "['achievers', 'afraid', 'challenge', 'challenges', 'comfortable', 'grow', 'makes', 'relish', 'stronger', 'thrive', 'uncomfort', 'uncomfortable', 'use']\n",
      "[[3 1 0 1 0 0 0 1 0 1 0 0 1]\n",
      " [0 0 0 1 0 0 1 0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
      " [0 0 0 0 1 1 0 0 0 0 1 0 0]\n",
      " [0 0 2 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# printing the vocabulary and the frequency distribution pf vocabulary in tokinzed sentences\n",
    "print(cv1.vocabulary_)\n",
    "print(sorted(cv1.vocabulary_))\n",
    "print(text_counts.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 1 sentence\n",
      ">>>>>>>>>>>>>>>>>\n",
      "achievers : 3\n",
      "afraid : 1\n",
      "challenges : 1\n",
      "relish : 1\n",
      "thrive : 1\n",
      "use : 1\n",
      "makes : 0\n",
      "stronger : 0\n",
      "uncomfortable : 0\n",
      "comfortable : 0\n",
      "uncomfort : 0\n",
      "grow : 0\n",
      "challenge : 0\n",
      "For 2 sentence\n",
      ">>>>>>>>>>>>>>>>>\n",
      "achievers : 0\n",
      "afraid : 0\n",
      "challenges : 1\n",
      "relish : 0\n",
      "thrive : 0\n",
      "use : 0\n",
      "makes : 1\n",
      "stronger : 1\n",
      "uncomfortable : 0\n",
      "comfortable : 0\n",
      "uncomfort : 0\n",
      "grow : 0\n",
      "challenge : 0\n",
      "For 3 sentence\n",
      ">>>>>>>>>>>>>>>>>\n",
      "achievers : 0\n",
      "afraid : 0\n",
      "challenges : 1\n",
      "relish : 0\n",
      "thrive : 0\n",
      "use : 0\n",
      "makes : 1\n",
      "stronger : 0\n",
      "uncomfortable : 1\n",
      "comfortable : 0\n",
      "uncomfort : 0\n",
      "grow : 0\n",
      "challenge : 0\n",
      "For 4 sentence\n",
      ">>>>>>>>>>>>>>>>>\n",
      "achievers : 0\n",
      "afraid : 0\n",
      "challenges : 0\n",
      "relish : 0\n",
      "thrive : 0\n",
      "use : 0\n",
      "makes : 0\n",
      "stronger : 0\n",
      "uncomfortable : 0\n",
      "comfortable : 1\n",
      "uncomfort : 1\n",
      "grow : 1\n",
      "challenge : 0\n",
      "For 5 sentence\n",
      ">>>>>>>>>>>>>>>>>\n",
      "achievers : 0\n",
      "afraid : 0\n",
      "challenges : 0\n",
      "relish : 0\n",
      "thrive : 0\n",
      "use : 0\n",
      "makes : 0\n",
      "stronger : 0\n",
      "uncomfortable : 0\n",
      "comfortable : 0\n",
      "uncomfort : 0\n",
      "grow : 0\n",
      "challenge : 2\n"
     ]
    }
   ],
   "source": [
    "for index,sentence in enumerate(text_counts.toarray()):\n",
    "    print(f\"For {index+1} sentence\")\n",
    "    print(\">>>>>>>>>>>>>>>>>\")\n",
    "    for keys,items in cv1.vocabulary_.items():\n",
    "        print(f\"{keys} : {sentence[items]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><br>TF-IDF (Term Frequency-Inverse Document Frequency)</h1></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term Frequency (TF)\n",
    "The Term Frequency (TF) is a measure of how frequently a term \n",
    "ùë°\n",
    "t appears in a document \n",
    "ùëë\n",
    "d. It can be calculated using the following formula:\n",
    "\n",
    "TF\n",
    "(\n",
    "ùë°\n",
    ",\n",
    "ùëë\n",
    ")\n",
    "=\n",
    "ùëì\n",
    "ùë°\n",
    ",\n",
    "ùëë\n",
    "ùëÅ\n",
    "ùëë\n",
    "TF(t,d)= \n",
    "N \n",
    "d\n",
    "‚Äã\n",
    " \n",
    "f \n",
    "t,d\n",
    "‚Äã\n",
    " \n",
    "‚Äã\n",
    " \n",
    "\n",
    "where:\n",
    "\n",
    "ùëì\n",
    "ùë°\n",
    ",\n",
    "ùëë\n",
    "f \n",
    "t,d\n",
    "‚Äã\n",
    "  is the number of times term \n",
    "ùë°\n",
    "t appears in document \n",
    "ùëë\n",
    "d.\n",
    "ùëÅ\n",
    "ùëë\n",
    "N \n",
    "d\n",
    "‚Äã\n",
    "  is the total number of terms in document \n",
    "ùëë\n",
    "d.\n",
    "Inverse Document Frequency (IDF)\n",
    "The Inverse Document Frequency (IDF) is a measure of how important a term \n",
    "ùë°\n",
    "t is across the entire corpus. It is calculated as follows:\n",
    "\n",
    "IDF\n",
    "(\n",
    "ùë°\n",
    ")\n",
    "=\n",
    "log\n",
    "‚Å°\n",
    "(\n",
    "ùëÅ\n",
    "1\n",
    "+\n",
    "ùëõ\n",
    "ùë°\n",
    ")\n",
    "+\n",
    "1\n",
    "IDF(t)=log( \n",
    "1+n \n",
    "t\n",
    "‚Äã\n",
    " \n",
    "N\n",
    "‚Äã\n",
    " )+1\n",
    "\n",
    "where:\n",
    "\n",
    "ùëÅ\n",
    "N is the total number of documents in the corpus.\n",
    "ùëõ\n",
    "ùë°\n",
    "n \n",
    "t\n",
    "‚Äã\n",
    "  is the number of documents containing the term \n",
    "ùë°\n",
    "t.\n",
    "The addition of 1 in the denominator is used to prevent division by zero (in case a term does not appear in any document).\n",
    "\n",
    "TF-IDF Calculation\n",
    "The TF-IDF value for a term \n",
    "ùë°\n",
    "t in a document \n",
    "ùëë\n",
    "d is then calculated by multiplying the TF and IDF values:\n",
    "\n",
    "TF-IDF\n",
    "(\n",
    "ùë°\n",
    ",\n",
    "ùëë\n",
    ")\n",
    "=\n",
    "TF\n",
    "(\n",
    "ùë°\n",
    ",\n",
    "ùëë\n",
    ")\n",
    "√ó\n",
    "IDF\n",
    "(\n",
    "ùë°\n",
    ")\n",
    "TF-IDF(t,d)=TF(t,d)√óIDF(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 9, 'cycle': 1, 'is': 5, 'ridden': 7, 'on': 6, 'track': 10, 'bus': 0, 'he': 4, 'driven': 2, 'road': 8, 'driving': 3}\n",
      "['bus', 'cycle', 'driven', 'driving', 'he', 'is', 'on', 'ridden', 'road', 'the', 'track']\n",
      "[1.28768207 1.69314718 1.69314718 1.69314718 1.28768207 1.\n",
      " 1.28768207 1.69314718 1.69314718 1.         1.69314718]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents\n",
    "text = [\"The cycle is ridden on the track.\",\n",
    "\t\"The bus he is driven on the road.\",\n",
    "\t\"He  is driving the bus.\"]\n",
    "\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "print(sorted(vectorizer.vocabulary_))\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the : 1.0\n",
      "cycle : 1.6931471805599454\n",
      "is : 1.0\n",
      "ridden : 1.6931471805599454\n",
      "on : 1.2876820724517808\n",
      "track : 1.6931471805599454\n",
      "bus : 1.2876820724517808\n",
      "he : 1.2876820724517808\n",
      "driven : 1.6931471805599454\n",
      "road : 1.6931471805599454\n",
      "driving : 1.6931471805599454\n"
     ]
    }
   ],
   "source": [
    "for keys,items in vectorizer.vocabulary_.items():\n",
    "    print(f\"{keys} : {vectorizer.idf_[items]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasttext-wiki-news-subwords-300\n",
      "conceptnet-numberbatch-17-06-300\n",
      "word2vec-ruscorpora-300\n",
      "word2vec-google-news-300\n",
      "glove-wiki-gigaword-50\n",
      "glove-wiki-gigaword-100\n",
      "glove-wiki-gigaword-200\n",
      "glove-wiki-gigaword-300\n",
      "glove-twitter-25\n",
      "glove-twitter-50\n",
      "glove-twitter-100\n",
      "glove-twitter-200\n",
      "__testing_word2vec-matrix-synopsis\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.downloader\n",
    " \n",
    "for model_name in list(gensim.downloader.info()['models'].keys()):\n",
    "  print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader\n",
    " \n",
    "google_news_vectors = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.07080078,  0.09472656,  0.10302734,  0.17285156, -0.13378906,\n",
       "       -0.07080078,  0.06933594, -0.21875   , -0.11230469,  0.06225586,\n",
       "       -0.05639648, -0.26171875, -0.21289062,  0.1328125 , -0.13574219,\n",
       "       -0.09472656,  0.06982422,  0.09960938, -0.06494141, -0.16601562,\n",
       "        0.0072937 , -0.11376953,  0.23339844, -0.12158203, -0.03271484,\n",
       "        0.03369141, -0.31054688, -0.02990723,  0.0456543 , -0.08447266,\n",
       "        0.13769531, -0.15039062, -0.22460938, -0.01519775, -0.19726562,\n",
       "       -0.04663086, -0.19824219, -0.00279236,  0.14257812,  0.03735352,\n",
       "        0.03442383, -0.078125  ,  0.11376953,  0.109375  ,  0.14160156,\n",
       "       -0.10253906,  0.05981445, -0.10742188, -0.14257812, -0.01708984,\n",
       "       -0.11914062,  0.00335693,  0.24511719,  0.09326172, -0.22851562,\n",
       "        0.02099609, -0.12792969, -0.04077148,  0.06176758,  0.01190186,\n",
       "        0.02148438,  0.0255127 ,  0.01000977,  0.04370117, -0.07910156,\n",
       "       -0.19433594,  0.16503906, -0.10888672,  0.08056641,  0.18261719,\n",
       "       -0.03015137,  0.18652344, -0.0100708 ,  0.00189972,  0.03112793,\n",
       "       -0.1875    , -0.05859375, -0.01037598,  0.10302734, -0.07177734,\n",
       "       -0.17285156,  0.17480469,  0.234375  , -0.10498047,  0.10986328,\n",
       "        0.05883789,  0.00430298, -0.13671875,  0.09619141,  0.11230469,\n",
       "       -0.03491211, -0.08740234, -0.16992188, -0.08398438, -0.09814453,\n",
       "        0.03930664,  0.19335938, -0.09960938,  0.33984375, -0.06201172,\n",
       "       -0.18847656, -0.03173828,  0.08642578,  0.01049805, -0.03393555,\n",
       "        0.1484375 , -0.12988281,  0.05981445,  0.15722656,  0.05957031,\n",
       "       -0.01989746, -0.1171875 , -0.02331543, -0.06347656,  0.02856445,\n",
       "       -0.03588867, -0.04101562, -0.2578125 ,  0.12597656,  0.00952148,\n",
       "        0.125     ,  0.13085938, -0.15722656,  0.01806641, -0.09082031,\n",
       "        0.24511719, -0.21875   ,  0.07128906,  0.07519531, -0.03710938,\n",
       "       -0.04785156,  0.09179688,  0.16992188,  0.20019531, -0.19921875,\n",
       "       -0.11621094, -0.19238281, -0.00366211,  0.12695312,  0.14160156,\n",
       "        0.19140625, -0.12988281,  0.14355469, -0.02246094, -0.03039551,\n",
       "        0.00613403, -0.0378418 , -0.125     ,  0.09716797, -0.09277344,\n",
       "        0.34570312, -0.17871094, -0.21679688, -0.14648438,  0.03063965,\n",
       "       -0.33007812, -0.16503906,  0.07080078,  0.08544922, -0.17871094,\n",
       "       -0.02844238, -0.01379395, -0.04907227, -0.10498047, -0.05957031,\n",
       "       -0.2890625 ,  0.02453613, -0.02209473, -0.1953125 ,  0.09619141,\n",
       "       -0.20214844, -0.04443359,  0.1015625 ,  0.0246582 ,  0.06787109,\n",
       "        0.02770996,  0.23242188, -0.19335938,  0.10644531, -0.28125   ,\n",
       "        0.0168457 ,  0.01660156,  0.01550293, -0.00765991,  0.03088379,\n",
       "        0.13183594,  0.13867188,  0.0402832 ,  0.18164062,  0.06542969,\n",
       "        0.01397705,  0.01647949, -0.27929688,  0.109375  ,  0.359375  ,\n",
       "        0.05395508, -0.00376892,  0.11914062, -0.10058594, -0.08642578,\n",
       "        0.04394531, -0.00854492, -0.02490234, -0.14453125, -0.00376892,\n",
       "        0.03979492,  0.02380371, -0.17285156, -0.16894531,  0.13964844,\n",
       "        0.06591797,  0.04736328, -0.14941406, -0.23339844, -0.18164062,\n",
       "        0.07714844,  0.17480469,  0.05151367, -0.25      ,  0.03808594,\n",
       "        0.06689453, -0.14746094,  0.11181641,  0.2734375 ,  0.05810547,\n",
       "        0.03173828, -0.01159668,  0.14941406,  0.10498047,  0.12792969,\n",
       "       -0.03442383, -0.08251953,  0.12988281,  0.02856445,  0.18457031,\n",
       "        0.05273438,  0.1640625 ,  0.09814453,  0.11669922,  0.21582031,\n",
       "        0.11767578,  0.13867188,  0.17675781,  0.2578125 , -0.20996094,\n",
       "        0.0625    , -0.23339844, -0.01403809, -0.02941895, -0.1796875 ,\n",
       "        0.15722656,  0.03808594,  0.23046875,  0.27929688,  0.08154297,\n",
       "       -0.09960938, -0.0043335 , -0.05297852,  0.09375   , -0.19335938,\n",
       "       -0.00099182,  0.17773438,  0.14257812, -0.03564453, -0.05419922,\n",
       "        0.11230469, -0.006073  , -0.01544189,  0.01745605, -0.04443359,\n",
       "       -0.15527344, -0.05615234,  0.04052734,  0.12011719,  0.15332031,\n",
       "       -0.05273438,  0.11816406,  0.0123291 , -0.27148438,  0.23339844,\n",
       "       -0.1640625 ,  0.12353516,  0.08300781, -0.12255859,  0.25390625,\n",
       "       -0.1953125 , -0.10693359,  0.125     , -0.06884766, -0.04541016,\n",
       "       -0.01586914,  0.03149414,  0.15820312, -0.06835938, -0.13378906,\n",
       "        0.00236511, -0.28710938,  0.05078125,  0.05126953,  0.34375   ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_vectors['nepal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding Capital of Britain: (Paris - France) + Nepal\n",
      "[('Kathmandu', 0.7831675410270691)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding Capital of Britain given Capital of France: (Paris - France) + Britain = \n",
    "print(\"Finding Capital of Britain: (Paris - France) + Nepal\")\n",
    "capital = google_news_vectors.most_similar([\"Paris\", \"Nepal\"], [\"France\"], topn=1)\n",
    "print(capital)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding Capital of India: (Berlin - Germany) + India\n",
      "[('Delhi', 0.7268317937850952)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding Capital of India given Capital of Germany: (Berlin - Germany) + India = \n",
    "print(\"Finding Capital of India: (Berlin - Germany) + India\")\n",
    "capital = google_news_vectors.most_similar([\"Berlin\", \"India\"], [\"Germany\"], topn=1)\n",
    "print(capital)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 similar words to BMW:\n",
      "('Audi', 0.7932199835777283)\n",
      "('Mercedes_Benz', 0.7683466672897339)\n",
      "('Porsche', 0.7272197604179382)\n",
      "('Mercedes', 0.7078384160995483)\n",
      "('Volkswagen', 0.6959410905838013)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding words similar to BMW\n",
    "print(\"5 similar words to BMW:\")\n",
    "words = google_news_vectors.most_similar(\"BMW\", topn=5)\n",
    "for word in words:\n",
    "    print(word)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 similar words to beautiful:\n",
      "('gorgeous', 0.8353005051612854)\n",
      "('lovely', 0.8106936812400818)\n",
      "('stunningly_beautiful', 0.7329413294792175)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding words similar to Beautiful\n",
    "print(\"3 similar words to beautiful:\")\n",
    "words = google_news_vectors.most_similar(\"beautiful\", topn=3)\n",
    "for word in words:\n",
    "      print(word)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between fight and battle: 0.7021284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding cosine similarity between fight and battle\n",
    "cosine = google_news_vectors.similarity(\"fight\", \"battle\")\n",
    "print(\"Cosine similarity between fight and battle:\", cosine)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between fight and love: 0.13506128\n"
     ]
    }
   ],
   "source": [
    "# Finding cosine similarity between fight and love\n",
    "cosine = google_news_vectors.similarity(\"fight\", \"love\")\n",
    "print(\"Cosine similarity between fight and love:\", cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><br>GloVe</h1><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasttext-wiki-news-subwords-300\n",
      "conceptnet-numberbatch-17-06-300\n",
      "word2vec-ruscorpora-300\n",
      "word2vec-google-news-300\n",
      "glove-wiki-gigaword-50\n",
      "glove-wiki-gigaword-100\n",
      "glove-wiki-gigaword-200\n",
      "glove-wiki-gigaword-300\n",
      "glove-twitter-25\n",
      "glove-twitter-50\n",
      "glove-twitter-100\n",
      "glove-twitter-200\n",
      "__testing_word2vec-matrix-synopsis\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.downloader\n",
    " \n",
    "for model_name in list(gensim.downloader.info()['models'].keys()):\n",
    "    print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.downloader\n",
    " \n",
    "google_news_vectors = gensim.downloader.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('eating', 0.8706035017967224), ('ate', 0.8597103357315063), ('eaten', 0.8458368182182312), ('eats', 0.7801411747932434), ('meal', 0.7678447365760803), ('chicken', 0.7672050595283508), ('meat', 0.7620247602462769), ('cooked', 0.7545220851898193), ('fish', 0.7533289790153503), ('drink', 0.7511290907859802)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding similar words\n",
    "similar = google_news_vectors.most_similar(\"eat\", topn=10)\n",
    "print(similar)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector = google_news_vectors[\"eat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.4295e-01, -4.2946e-01, -5.4277e-01, -1.0307e+00,  1.2056e+00,\n",
       "       -2.7174e-01, -6.3561e-01, -1.5065e-02,  3.7856e-01,  4.6474e-02,\n",
       "       -1.3102e-01,  6.0500e-01,  1.6391e+00,  2.3940e-01,  1.2128e+00,\n",
       "        8.3178e-01,  7.3893e-01,  1.5200e-01, -1.4175e-01, -8.8384e-01,\n",
       "        2.0829e-02, -3.2545e-01,  1.8035e+00,  1.0045e+00,  5.8484e-01,\n",
       "       -6.2031e-01, -4.3296e-01,  2.3562e-01,  1.3027e+00, -8.1264e-01,\n",
       "        2.3158e+00,  1.1030e+00, -6.0608e-01,  1.0101e+00, -2.2426e-01,\n",
       "        1.8908e-02, -1.0931e-01,  3.8350e-01,  7.7362e-01, -8.1927e-02,\n",
       "       -3.4040e-01, -1.5143e-03, -5.6640e-02,  8.7359e-01,  1.4805e+00,\n",
       "        6.9421e-01, -3.0966e-01, -9.0826e-01,  3.7277e-03,  8.4550e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('class')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "2ab26ebc9eac352ad84074c5872f53e9ff2d0f751ba36ba2a556974c64f5628d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
